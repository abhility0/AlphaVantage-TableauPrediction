{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1358750-b9f2-4dfb-a012-5721c5c68ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from io import StringIO\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- S3 and Data Configuration ---\n",
    "S3_BUCKET = 'alpha-stock'        \n",
    "DATA_PREFIX = 'alphavantage_data/'     \n",
    "PREDICTIONS_OUTPUT_KEY = 'predictions/predicted_stock_prices.json'\n",
    "\n",
    "# Initialize the S3 client \n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def load_data_from_s3(bucket, prefix=''):\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    data_frames = []\n",
    "    \n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        if key.endswith('.json'):\n",
    "            obj_body = s3.get_object(Bucket=bucket, Key=key)['Body'].read().decode('utf-8')\n",
    "            data_json = json.loads(obj_body)\n",
    "            if \"Time Series (1min)\" in data_json:\n",
    "                ts_data = data_json[\"Time Series (1min)\"]\n",
    "                df = pd.DataFrame.from_dict(ts_data, orient='index')\n",
    "                # Convert all columns to float and reset index as timestamp\n",
    "                df = df.astype(float)\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "                df.sort_index(inplace=True)\n",
    "                data_frames.append(df)\n",
    "    \n",
    "    if data_frames:\n",
    "        combined_df = pd.concat(data_frames)\n",
    "        combined_df.sort_index(inplace=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No JSON files found in S3 matching the criteria.\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(df):\n",
    "    data = df['4. close'].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    return scaled_data, scaler\n",
    "\n",
    "def create_dataset(dataset, time_step=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        a = dataset[i:(i + time_step), 0]\n",
    "        X.append(a)\n",
    "        y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model(time_step):\n",
    "    \"\"\"\n",
    "    Build and compile a simple LSTM network.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def upload_predictions_to_s3(bucket, key, predictions):\n",
    "    predictions_json = json.dumps(predictions)\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=predictions_json)\n",
    "    print(f\"Uploaded predictions to s3://{bucket}/{key}\")\n",
    "\n",
    "def main():\n",
    "    # --- Step 1: Load Data ---\n",
    "    df = load_data_from_s3(S3_BUCKET, DATA_PREFIX)\n",
    "    if df is None:\n",
    "        return\n",
    "    print(\"Data loaded from S3. Total records:\", len(df))\n",
    "    \n",
    "    # --- Step 2: Preprocess Data ---\n",
    "    scaled_data, scaler = preprocess_data(df)\n",
    "    time_step = 60  # Use 60 timesteps (adjust based on your data frequency)\n",
    "    X, y = create_dataset(scaled_data, time_step)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    \n",
    "    # --- Step 3: Split Data for Training and Testing ---\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # --- Step 4: Build and Train the LSTM Model ---\n",
    "    model = build_model(time_step)\n",
    "    model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # --- Step 5: Predict Future Price ---\n",
    "    # Use the last 'time_step' records to predict the next closing price\n",
    "    last_sequence = scaled_data[-time_step:].reshape(1, time_step, 1)\n",
    "    predicted_price_scaled = model.predict(last_sequence)\n",
    "    predicted_price = scaler.inverse_transform(predicted_price_scaled)\n",
    "    \n",
    "    predictions = {\n",
    "        \"predicted_price\": predicted_price[0][0],\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "    }\n",
    "    print(\"Predicted Stock Price:\", predictions[\"predicted_price\"])\n",
    "    \n",
    "    # --- Step 6: Upload Predictions to S3 ---\n",
    "    upload_predictions_to_s3(S3_BUCKET, PREDICTIONS_OUTPUT_KEY, predictions)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
